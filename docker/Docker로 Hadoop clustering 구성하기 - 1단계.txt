원숭이도 할 수 있는 도커를 이용하여 하둡 클러스터 구성하기!!!

on-premise 로 하둡 클러스터를 구성하기에 앞서 첫번째 단계로 도커를 이용하여 하둡 클러스터를 구성하고 스프링 배치를 이용하여 
hdfs 에 file write 까지 테스트를 진행하였다.
컨테이너간의 네트워크는 link 옵션을 사용하였으며 bridge, docker-compose, docker-swarm, overlay, kubernetes 를 적용하여
발전시켜 나갈 계획이다.


***********************************************************************************************************************


리눅스의 패키지 관리툴로써 Red Hat 계열에서는 yum 을 사용하고 Debian 에서는 apt 를 사용하다.  우분투는 apt 를 사용한다.

먼저 하둡을 설치할 리눅스 컨테이너의 이미지를 다운받아 생성하고 실행한다.


-----------------------------------------------------------------------------------------------------------------------
$ docker run -it --name hadoop-base ubuntu
-----------------------------------------------------------------------------------------------------------------------
run    : 이미지를 pull 하고 컨테이너를 create 한고 생성한 도커 컨테이너를 start 한다.
-it    : interactive + tty : tty 가상 터미널을 이용하여 컨테이너와 실시간 통신한다.
--name : name 에 기술된 컨테이너 이름으로 이미지를 실행시킨다.  name 을 기술하지 않으면 도커가 임의의 이름을 부여한다.
ubuntu : 실행시킬 이미지 이름.  로컬에 없을 경우 도커 공식 레파지토리에서 이미지를 pull 하게 된다. 
-----------------------------------------------------------------------------------------------------------------------


* 도커 이미지는 실행시킬 프로그램을 패키지화 한 파일이라고 보면 되고, 이 이미지를 실행시킨 프로세스를 컨테이너라고 한다.
컨테이너는 docker ps -a 로 조회 할 수 있으며, 실행이 끝난 컨테이너라고 하더라도 저절로 삭제되지는 않고 exit 상태로 남아 있게 된다.

* 우분투 -it 옵션을 이용하여 컨테이너를 실행시키면 현재의 shell 은 호스트 shell 에서 우분투 shell 로 되고 따라서 프롬프트도 
우분트 프롬프트가 나타나게 된다.


현재 실행시킨 우분투 리눅스는 깡통인 상태이므로 하둡 설치에 필요한 프로그램들은 apt 툴을 이용하여 설치한다.


-----------------------------------------------------------------------------------------------------------------------
# apt-get update
-----------------------------------------------------------------------------------------------------------------------


자바를 설치하고 설치 디렉토리를 확인한다.


-----------------------------------------------------------------------------------------------------------------------
# apt-get install openjdk-8-jdk -y
# ls /usr/lib/jvm/java-8-openjdk-amd64
-----------------------------------------------------------------------------------------------------------------------


하둡 파일을 받기 위해 wget 을 설치한다.


-----------------------------------------------------------------------------------------------------------------------
# apt-get install wget -y
-----------------------------------------------------------------------------------------------------------------------


설정파일등을 편집하기 위해 vi 에디터도 설치한다.


-----------------------------------------------------------------------------------------------------------------------
# apt-get install vim -y
-----------------------------------------------------------------------------------------------------------------------


ssh 는 키교환 방식을 이용하여 인증된 노드(클러스터)간에 별도의 인증과정없이 접속이 가능하게 하는 프로그램이다.
하둡을 클러스터로 구성하기 위해서는 ssh 설정을 해주어야 한다.


-----------------------------------------------------------------------------------------------------------------------
# apt-get install ssh -y

# ssh-keygen -t rsa -P '' -f ~/.ssh/id_dsa
# cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
# mkdir /var/run/sshd
# /usr/sbin/sshd
-----------------------------------------------------------------------------------------------------------------------


ssh 를 설치하고 키 파일을 접속할 때 사용하는 계정의 홈 디렉토리에 복사해 준다.  -u 옵션을 주지 않으면 디폴트로 root 계정으로 
도커가 실행되므로 해당 홈디렉토리 아래에 키파일을 복사한다.
ssh 데몬이 사용하는 디렉토리도 생성해 준다.
셋팅이 완료된 후 필요한 컨테이너를 모두 기동시킨뒤 ssh 작동 여부 확인은 후술한다.


하둡 홈 디렉토리를 생성하고 바이너리를 다운로드한다.  바이너리 압출을 풀기만 하면 설치는 끝난다.  참고로 하둡 소스를 다운받아 
직접 컴파일하는 방법을 사용할 수도 있다.


-----------------------------------------------------------------------------------------------------------------------
# mkdir /hadoop_home
# cd /hadoop_home
# wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz
# tar xvzf hadoop-2.7.7.tar.gz
-----------------------------------------------------------------------------------------------------------------------


컨테이너 실행 유저의 환경변수를 세팅한다.  유저의 홈디렉토리의 .bashrc 의 내용은 해당 유저가 로그인하게 
되면(컨테이너를 run 하게 되면) 자동으로 적용된다.


-----------------------------------------------------------------------------------------------------------------------
# cd
(로그인 유저-root-의 홈디렉토리로 이동)
# vi ~/.bashrc
(아래내용을 기술)
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export HADOOP_HOME=/hadoop_home/hadoop-2.7.7
export HADOOP_CONFIG_HOME=$HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
# source ~/.bashrc
(다시 로그인을 하지 않아도 현재의 .bashrc 내용이 적용된다)
-----------------------------------------------------------------------------------------------------------------------


하둡 클러스터가 사용할 naemenode 디렉토리와 datanode 디렉토를 만들어 준다.


-----------------------------------------------------------------------------------------------------------------------
# mkdir /hadoop_home/temp
# mkdir /hadoop_home/namenode_home
# mkdir /hadoop_home/datanode_home
-----------------------------------------------------------------------------------------------------------------------


하둡 설정 파일들을 수정한다.  참고로 하둡을 처음 설치하게 되면 mapred-site.xml.template 파일만 있으므로 이 파일을 mapred-site.xml 파일로 복사하여 사용한다.


-----------------------------------------------------------------------------------------------------------------------
# cd $HADOOP_CONFIG_HOME
# cp mapred-site.xml.template mapred-site.xml
-----------------------------------------------------------------------------------------------------------------------


HADOOP_CONFIG_HOME 아래의 파일의 configuration 태그를 다음과 같이 수정한다.


-----------------------------------------------------------------------------------------------------------------------
core-site.xml
-----------------------------------------------------------------------------------------------------------------------
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/hadoop_home/temp</value>
    </property>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://master:9000</value>
        <final>true</final>
    </property>
</configuration>
-----------------------------------------------------------------------------------------------------------------------
하둡의 hdfs를 이용하는 클라이언트가 사용해야 하는 hdfs 주소를 기술한다.
ex) new URI("hdfs://master:9000")
-----------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------
hdfs-site.xml
-----------------------------------------------------------------------------------------------------------------------
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
        <final>true</final>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>/hadoop_home/namenode_home</value>
        <final>true</final>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>/hadoop_home/datanode_home</value>
        <final>true</final>
    </property>

    <!-- 데이터 노드에서도 hostname 을 사용하도록 설정.  client 와 맞춰워야 함 -->
    <!--
    ex) Configuration conf = new Configuration();
        conf.set("dfs.client.use.datanode.hostname", "true");
    -->
    <property>
        <name>dfs.client.use.datanode.hostname</name> 
        <value>true</value>
    </property>
    
    <!-- permission 제한을 없앰 -->
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
</configuration>
-----------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------
mapred-site.xml
-----------------------------------------------------------------------------------------------------------------------
<configuration>
    <property>
        <name>mapred.job.tracker</name>
        <value>master:9001</value>
    </property>
</configuration>
-----------------------------------------------------------------------------------------------------------------------


클라이언트 프로그램에서 hdfs 를 사용할때는 core-site.xml 의 fs.default.name 을 이용하여 접속을 하게 되고, 
일단 connection 이 완료된 후 datanode 입출력시에는 datanode 의 50075 포트를 이용한다.
이를 위해 하둡은 클라이언트가 데이터노드에 접근할 수 있도록 datanode 의 IP를 클라이언트에 전달하게 되는데 이때 데이터 노드의
가상IP를 전달하게 된다.  하지만 클라이언트는 가상 IP로의 접근이 불가능하기 때문에 결국 Connection Timeout 예외가 발생하게 된다.  
따라서 데이터 노드 또한 호스트 이름으로 접근할 수 있도록 hdfs-site.xml 에 dfs.client.use.datanode.hostname 옵션을 추가한다.

이러한 현상은 하둡에 내장되어 있는 Admin 페이지(localhost:50070) 에서도 마찬가지 이다.  네임노드의 hostname 을 이용하여 
admin 페이지로의 접속은 가능하지만, admin 페이지를 통해 데이터 노드에 접근하는 경우(파일 다운로드 등)에는 위에서 기술한 이유로 
Connection Timeout 예외가 발생하게 되니다.  이 때는 호스트의 hosts 파일에 아래와 같은 alias 를 기술함으로써 위와 같은 문제를 
피할 수 있다. 


-----------------------------------------------------------------------------------------------------------------------
hosts
-----------------------------------------------------------------------------------------------------------------------
127.0.0.1    master
127.0.0.1    slave1
127.0.0.1    slave2
-----------------------------------------------------------------------------------------------------------------------
admin 페이지에서 데이터 노드에 접근하는 경우 slave1, slave2 를 이용하게 된다.
따라서 slave1, slave2 에 대한 실제 IP를 alias 를 해주어야 파일 다운로드와 같은 데이터 노드로의 접근이 가능하다.
-----------------------------------------------------------------------------------------------------------------------


마지막으로 hadoop-env.sh 파일에 JAVA_HOME 위치를 지정한다.


-----------------------------------------------------------------------------------------------------------------------
hadoop-env.sh
-----------------------------------------------------------------------------------------------------------------------
(...)
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
-----------------------------------------------------------------------------------------------------------------------


셋팅은 완료되었고 이제 네임노드를 포맷한다.
-----------------------------------------------------------------------------------------------------------------------
# hadoop namenode -format
-----------------------------------------------------------------------------------------------------------------------


'SHUTDOWN_MSG: Shutting down NameNode at ...' 이런식의 메시지가 뜨면 정상적으로 완료된 것이다.


이제 지금까지 설정한 내용을 이미지로 보관하기 위해 docker commit 을 실행하고 repository 에 보관한다.  docker commit을 실행하면 
현재 컨테이너를 이미지로 저장하게 되고 push 를 통해 reposotory 에 올리게 되면 다른 호스트에서도 해당 이미지를 사용할 수 있다.  
컨테이너를 종료하고 커밋을 수행한다.


-----------------------------------------------------------------------------------------------------------------------
# exit
$ docker commit hadoop-base ubuntu:hadoop
-----------------------------------------------------------------------------------------------------------------------
hadoop-base   : 이미지로 저장할 컨테이너 이름
ubuntu:hadoop : 생성할 이미지 이름(레파지토리):태그
-----------------------------------------------------------------------------------------------------------------------


도커에 push 하기 전에 사용자가 정의한 tag 를 붙여줘야 한다.


-----------------------------------------------------------------------------------------------------------------------
$ docker login
$ docker tag ubuntu:hadoop leehyounkyoo/ubuntu:hadoop
$ docker push leehyounkyoo/ubuntu:hadoop
-----------------------------------------------------------------------------------------------------------------------
ubuntu:hadoop              : 생성한 이미지 이름(레파지토리):태그
leehyounkyoo/ubuntu:hadoop : 유저ID/푸쉬할 이미지 이름(레파지토리):태그
-----------------------------------------------------------------------------------------------------------------------


지금까지 셋팅한 컨테이너를 이미지로 만들어 push 해서 저장해 놓고, 이제 실제로 지금까지 셋팅한 이미지를 이용하여 
하둡 클러스터링을 위한 컨테이너를 띄운다.

같은 이미지라도 다른 이름으로 컨테이너를 만들게 되면 상괸이 없으나 깔끔하게 진행하게 하기 위해 지금까지 셋팅한 컨테이너를 
깔끔하게 지운다.


-----------------------------------------------------------------------------------------------------------------------
$ docker rm hadoop-base
-----------------------------------------------------------------------------------------------------------------------


마스터 노드, 슬레이브 노드 순서대로 컨테이너를 생성한다.  슬레이브를 생성할 때 슬레이브 번호 순서대로 컨테이를 생성해야 
가상의 IP가 순서대로 부여되므로 주의한다.

또한 하둡은 기본적으로 사용하는 포트들이 많으니 해당 포트들로 모두 포워딩 해준다.


-----------------------------------------------------------------------------------------------------------------------
$ docker run -it -h master --name master -p 9000:9000 -p 50010:50010 -p 50020:50020 -p 50030:50030 -p 50060:50060 -p 50070:50070 -p 50075:50075 -p 50090:50090 ubuntu:hadoop
$ docker run -it --rm -h slave1 --name slave1 --link master:master ubuntu:hadoop
$ docker run -it --rm -h slave2 --name slave2 --link master:master ubuntu:hadoop
$ docker run -it --rm -h client --name client --link master:master ubuntu:hadoop
-----------------------------------------------------------------------------------------------------------------------
--rm                 : 컨테이너가 종료되면 자동 삭제된다.
-h master            : 컨테이너의 hostname 을 지정
--name master        : 컨테이너 이름.  지정을 안할 경우 도커가 임의의 이름을 부여한다(slave1, slave2, client 동일)
-p 9000:9000         : 외부 포트와 컨테이너간의 내부 포트를 맵핑한다.  
                       즉, 외부에서 9000 포트로 들어오는 경우 컨테이너의 9000 포트로 연결한다.
ubuntu:hadoop        : 컨테이너로 만들 이미지 이름
--link master:master : 현재의 컨테이너의 master 를 외부의 master 컨테이어로 맵핑한다.  이렇게 되면 컨테이너간 통신이 가능하다.
-----------------------------------------------------------------------------------------------------------------------


3개의 컨테이너의 가상IP 를 확인하여 master 컨테이너의 hosts 에 설정한다.


-----------------------------------------------------------------------------------------------------------------------
$ docker inspect master | grep IPAddress
$ docker inspect slave1 | grep IPAddress
$ docker inspect slave2 | grep IPAddress
-----------------------------------------------------------------------------------------------------------------------
ex) Master : 172.17.0.2
    Slave1 : 172.17.0.3
    Slave2 : 172.17.0.4
-----------------------------------------------------------------------------------------------------------------------


-----------------------------------------------------------------------------------------------------------------------
master 컨테이너 /etc/hosts
-----------------------------------------------------------------------------------------------------------------------
172.17.0.2 master
172.17.0.3 slave1
172.17.0.4 slave2
-----------------------------------------------------------------------------------------------------------------------


컨테이너가 정상적으로 실행되었으면 slave1, slave2 에 ssh 로 접속해 본다.  최초 접속시에는 yes 를 해준다.  만일 접속이 원할하지 
않을 경우 ssh 데몬(sshd)을 확인해보고 그래도 안되는 경우 ssh 를 다시 설정해 준다.  ssh 재설정을 할 경우에는 재설정 후 모든 
컨테이너를 껐다가 다시 실행시켜야 한다.


-----------------------------------------------------------------------------------------------------------------------
# ssh root@slave1
(exit)
# ssh root@slave2
(exit)
-----------------------------------------------------------------------------------------------------------------------


마스터 컨테이너 HADOOP_CONFIG_HOME 의 slaves 에 데이터 노드로 동작할 컨테이너 목록을 기술한다.


-----------------------------------------------------------------------------------------------------------------------
master 컨테이너 slaves
-----------------------------------------------------------------------------------------------------------------------
slave1
slave2
master
-----------------------------------------------------------------------------------------------------------------------


마스터 컨테이너에서 하둡을 기동시킨다.


-----------------------------------------------------------------------------------------------------------------------
# start-all.sh
-----------------------------------------------------------------------------------------------------------------------


네임 노드 및 데이터 노드의 자바 프로세스를 확인하거나 하둡 report 툴을 이용하여 정상여부를 확인한다.  
또는 Admin 화면(master:50070) 을 이용하여 확인 가능하다.


-----------------------------------------------------------------------------------------------------------------------
(마스터)
# jps
704 NodeManager
273 DataNode
440 SecondaryNameNode
138 NameNode
990 Jps
591 ResourceManager
 
(슬레이브)
# jps
245 Jps
149 NodeManager
39 DataNode

# hdfs dfsadmin -report
-----------------------------------------------------------------------------------------------------------------------


** Wordcount 를 이용하여 hdfs 테스트 하기 **


-----------------------------------------------------------------------------------------------------------------------
# docker run -it -h client --name client --link master:master ubuntu:hadoop
# cd $HADOOP_HOME
# hadoop fs -mkdir -p /test
# hadoop fs -put LICENSE.txt /test
# hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar wordcount /test /test_out

(MapReduce 실행...)

# hadoop fs -cat /test_out/*
# hadoop fs -ls /test_out
-----------------------------------------------------------------------------------------------------------------------


위의 결과는 Admin 화면의 Browse Directory 를 이용하여 확인 할 수도 있다.


* Java 를 이용하여 hdfs write 를 할때는 datanode 또한 hostname 을 이용하여 접근하도록 셋팅하여야 한다.


-----------------------------------------------------------------------------------------------------------------------
(...)
Configuration conf = new Configuration();
(...)
conf.set("dfs.client.use.datanode.hostname", "true");
(...)
FileSystem fs = FileSystem.get(new URI("hdfs://master:9000"), conf);
(...)
-----------------------------------------------------------------------------------------------------------------------

