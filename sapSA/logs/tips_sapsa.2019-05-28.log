2019:05:28 18:15:04.924 INFO  --- [restartedMain] k.c.t.sapsa.SapSaApplication : Starting SapSaApplication on ntsa with PID 1844 (C:\eclipse\workspace\sapSA\target\classes started by admin2 in C:\eclipse\workspace\sapSA) 
2019:05:28 18:15:04.939 INFO  --- [restartedMain] k.c.t.sapsa.SapSaApplication : No active profile set, falling back to default profiles: default 
2019:05:28 18:15:05.017 INFO  --- [restartedMain] o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext : Refreshing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@70dae9ef: startup date [Tue May 28 18:15:05 KST 2019]; root of context hierarchy 
2019:05:28 18:15:07.342 INFO  --- [restartedMain] o.s.c.s.PostProcessorRegistrationDelegate$BeanPostProcessorChecker : Bean 'org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration' of type [org.springframework.transaction.annotation.ProxyTransactionManagementConfiguration$$EnhancerBySpringCGLIB$$b87eaa79] is not eligible for getting processed by all BeanPostProcessors (for example: not eligible for auto-proxying) 
2019:05:28 18:15:07.998 INFO  --- [restartedMain] o.s.b.w.e.tomcat.TomcatWebServer : Tomcat initialized with port(s): 8090 (http) 
2019:05:28 18:15:08.045 INFO  --- [restartedMain] o.a.catalina.core.StandardService : Starting service [Tomcat] 
2019:05:28 18:15:08.060 INFO  --- [restartedMain] o.a.catalina.core.StandardEngine : Starting Servlet Engine: Apache Tomcat/8.5.31 
2019:05:28 18:15:08.060 INFO  --- [localhost-startStop-1] o.a.c.core.AprLifecycleListener : The APR based Apache Tomcat Native library which allows optimal performance in production environments was not found on the java.library.path: [C:\Program Files\Java\jre1.8.0_172\bin;C:\Windows\Sun\Java\bin;C:\Windows\system32;C:\Windows;C:/Program Files/Java/jre1.8.0_172/bin/server;C:/Program Files/Java/jre1.8.0_172/bin;C:/Program Files/Java/jre1.8.0_172/lib/amd64;C:\Program Files (x86)\Common Files\Oracle\Java\javapath;C:\Windows\system32;C:\Windows;C:\Windows\System32\Wbem;C:\Windows\System32\WindowsPowerShell\v1.0\;C:\Program Files\Java\jdk1.8.0_172\bin;;C:\eclipse;;.] 
2019:05:28 18:15:08.248 INFO  --- [localhost-startStop-1] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring embedded WebApplicationContext 
2019:05:28 18:15:08.263 INFO  --- [localhost-startStop-1] o.s.web.context.ContextLoader : Root WebApplicationContext: initialization completed in 3246 ms 
2019:05:28 18:15:08.591 INFO  --- [localhost-startStop-1] o.s.b.w.s.ServletRegistrationBean : Servlet dispatcherServlet mapped to [/] 
2019:05:28 18:15:08.591 INFO  --- [localhost-startStop-1] o.s.b.w.s.FilterRegistrationBean : Mapping filter: 'simpleCorsFilter' to: [/*] 
2019:05:28 18:15:08.591 INFO  --- [localhost-startStop-1] o.s.b.w.s.FilterRegistrationBean : Mapping filter: 'characterEncodingFilter' to: [/*] 
2019:05:28 18:15:08.591 INFO  --- [localhost-startStop-1] o.s.b.w.s.FilterRegistrationBean : Mapping filter: 'hiddenHttpMethodFilter' to: [/*] 
2019:05:28 18:15:08.591 INFO  --- [localhost-startStop-1] o.s.b.w.s.FilterRegistrationBean : Mapping filter: 'httpPutFormContentFilter' to: [/*] 
2019:05:28 18:15:08.591 INFO  --- [localhost-startStop-1] o.s.b.w.s.FilterRegistrationBean : Mapping filter: 'requestContextFilter' to: [/*] 
2019:05:28 18:15:08.951 INFO  --- [restartedMain] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Starting... 
2019:05:28 18:15:09.762 INFO  --- [restartedMain] com.zaxxer.hikari.pool.PoolBase : HikariPool-1 - Driver does not support get/set network timeout for connections. (Method getNetworkTimeout() of Connection is not supported.) 
2019:05:28 18:15:09.777 INFO  --- [restartedMain] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Start completed. 
2019:05:28 18:15:09.872 INFO  --- [restartedMain] o.s.o.j.LocalContainerEntityManagerFactoryBean : Building JPA container EntityManagerFactory for persistence unit 'default' 
2019:05:28 18:15:09.903 INFO  --- [restartedMain] o.h.jpa.internal.util.LogHelper : HHH000204: Processing PersistenceUnitInfo [
	name: default
	...] 
2019:05:28 18:15:10.044 INFO  --- [restartedMain] org.hibernate.Version : HHH000412: Hibernate Core {5.2.17.Final} 
2019:05:28 18:15:10.044 INFO  --- [restartedMain] org.hibernate.cfg.Environment : HHH000206: hibernate.properties not found 
2019:05:28 18:15:10.122 INFO  --- [restartedMain] o.h.annotations.common.Version : HCANN000001: Hibernate Commons Annotations {5.0.1.Final} 
2019:05:28 18:15:10.371 INFO  --- [restartedMain] org.hibernate.dialect.Dialect : HHH000400: Using dialect: org.hibernate.dialect.HANAColumnStoreDialect 
2019:05:28 18:15:10.402 INFO  --- [restartedMain] o.h.e.j.e.i.LobCreatorBuilderImpl : HHH000422: Disabling contextual LOB creation as connection was null 
2019:05:28 18:15:11.432 INFO  --- [restartedMain] o.s.o.j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default' 
2019:05:28 18:15:13.352 INFO  --- [restartedMain] o.s.w.s.h.SimpleUrlHandlerMapping : Mapped URL path [/**/favicon.ico] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler] 
2019:05:28 18:15:13.977 INFO  --- [restartedMain] o.s.w.s.m.m.a.RequestMappingHandlerAdapter : Looking for @ControllerAdvice: org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@70dae9ef: startup date [Tue May 28 18:15:05 KST 2019]; root of context hierarchy 
2019:05:28 18:15:14.055 WARN  --- [restartedMain] o.s.b.a.o.j.JpaBaseConfiguration$JpaWebConfiguration$JpaWebMvcConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning 
2019:05:28 18:15:14.133 INFO  --- [restartedMain] o.s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/elevator/breakComponent/{dId}],methods=[GET]}" onto public kr.co.tipsvalley.sapsa.httpEntity.RestResponseEntity<java.util.List<kr.co.tipsvalley.sapsa.model.json.ElevatorComponentDownInfo>> kr.co.tipsvalley.sapsa.controller.ElevatorController.getBreakComponentListByDeviceId(int) 
2019:05:28 18:15:14.133 INFO  --- [restartedMain] o.s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/elevator/msg/initSet],methods=[GET]}" onto public kr.co.tipsvalley.sapsa.httpEntity.RestBaseResponse kr.co.tipsvalley.sapsa.controller.ElevatorController.insertDeviceMonitoringSet() 
2019:05:28 18:15:14.133 INFO  --- [restartedMain] o.s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/elevator/liveData],methods=[GET]}" onto public kr.co.tipsvalley.sapsa.httpEntity.RestResponseEntity<java.util.List<kr.co.tipsvalley.sapsa.model.json.ElevatorBreakDownInfo>> kr.co.tipsvalley.sapsa.controller.ElevatorController.getElevatorRealData() 
2019:05:28 18:15:14.148 INFO  --- [restartedMain] o.s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/elevator/liveData2],methods=[GET]}" onto public kr.co.tipsvalley.sapsa.httpEntity.RestResponseEntity<java.util.List<kr.co.tipsvalley.sapsa.model.json.ElevatorBreakDownInfo>> kr.co.tipsvalley.sapsa.controller.ElevatorController.getElevatorRealData2() 
2019:05:28 18:15:14.148 INFO  --- [restartedMain] o.s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/elevator/device],methods=[GET]}" onto public kr.co.tipsvalley.sapsa.httpEntity.RestResponseEntity<java.util.List<kr.co.tipsvalley.sapsa.model.json.DeviceInfo>> kr.co.tipsvalley.sapsa.controller.ElevatorController.getElevatorDeviceData(kr.co.tipsvalley.sapsa.model.json.ElevatorSearchInfo) 
2019:05:28 18:15:14.148 INFO  --- [restartedMain] o.s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/elevator/statistics],methods=[GET]}" onto public kr.co.tipsvalley.sapsa.httpEntity.RestResponseEntity<kr.co.tipsvalley.sapsa.model.json.ElevatorData> kr.co.tipsvalley.sapsa.controller.ElevatorController.getElevatorStatisticsData(kr.co.tipsvalley.sapsa.model.json.ElevatorSearchInfo) 
2019:05:28 18:15:14.148 INFO  --- [restartedMain] o.s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/elevator/msg/insert],methods=[GET]}" onto public kr.co.tipsvalley.sapsa.httpEntity.RestBaseResponse kr.co.tipsvalley.sapsa.controller.ElevatorController.insertMsg() 
2019:05:28 18:15:14.148 INFO  --- [restartedMain] o.s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/elevator/msg/insert/rnd],methods=[GET]}" onto public kr.co.tipsvalley.sapsa.httpEntity.RestBaseResponse kr.co.tipsvalley.sapsa.controller.ElevatorController.insertMsgRandom() 
2019:05:28 18:15:14.148 INFO  --- [restartedMain] o.s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/kafka/data],methods=[GET]}" onto public kr.co.tipsvalley.sapsa.httpEntity.RestResponseEntity<java.util.List<kr.co.tipsvalley.sapsa.model.json.KafkaSensorInfo>> kr.co.tipsvalley.sapsa.controller.KafkaController.kafka(org.springframework.ui.Model) throws java.io.IOException 
2019:05:28 18:15:14.148 INFO  --- [restartedMain] o.s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/sensor/device],methods=[GET]}" onto public kr.co.tipsvalley.sapsa.httpEntity.RestResponseEntity<java.util.List<kr.co.tipsvalley.sapsa.model.json.DeviceInfo>> kr.co.tipsvalley.sapsa.controller.SensorController.getSensorDeviceData() 
2019:05:28 18:15:14.148 INFO  --- [restartedMain] o.s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/sensor/statistics],methods=[GET]}" onto public kr.co.tipsvalley.sapsa.httpEntity.RestResponseEntity<java.util.List<kr.co.tipsvalley.sapsa.model.json.SensorInfo>> kr.co.tipsvalley.sapsa.controller.SensorController.getSensorStatisticsData(kr.co.tipsvalley.sapsa.model.json.SearchInfo) 
2019:05:28 18:15:14.148 INFO  --- [restartedMain] o.s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/sensor/calendar],methods=[GET]}" onto public kr.co.tipsvalley.sapsa.httpEntity.RestResponseEntity<java.util.List<kr.co.tipsvalley.sapsa.model.json.SensorInfo>> kr.co.tipsvalley.sapsa.controller.SensorController.setCalendarData() 
2019:05:28 18:15:14.148 INFO  --- [restartedMain] o.s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/sensor/data],methods=[GET]}" onto public kr.co.tipsvalley.sapsa.httpEntity.RestResponseEntity<java.util.List<kr.co.tipsvalley.sapsa.model.json.SensorInfo>> kr.co.tipsvalley.sapsa.controller.SensorController.getSensorData() 
2019:05:28 18:15:14.164 INFO  --- [restartedMain] o.s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/user/menu/info],methods=[GET]}" onto public kr.co.tipsvalley.sapsa.httpEntity.RestResponseEntityList<kr.co.tipsvalley.sapsa.model.json.UserMenu> kr.co.tipsvalley.sapsa.controller.UserController.getUserMenuInfo() 
2019:05:28 18:15:14.164 INFO  --- [restartedMain] o.s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error]}" onto public org.springframework.http.ResponseEntity<java.util.Map<java.lang.String, java.lang.Object>> org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.error(javax.servlet.http.HttpServletRequest) 
2019:05:28 18:15:14.164 INFO  --- [restartedMain] o.s.w.s.m.m.a.RequestMappingHandlerMapping : Mapped "{[/error],produces=[text/html]}" onto public org.springframework.web.servlet.ModelAndView org.springframework.boot.autoconfigure.web.servlet.error.BasicErrorController.errorHtml(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse) 
2019:05:28 18:15:14.242 INFO  --- [restartedMain] o.s.w.s.h.SimpleUrlHandlerMapping : Mapped URL path [/webjars/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler] 
2019:05:28 18:15:14.242 INFO  --- [restartedMain] o.s.w.s.h.SimpleUrlHandlerMapping : Mapped URL path [/**] onto handler of type [class org.springframework.web.servlet.resource.ResourceHttpRequestHandler] 
2019:05:28 18:15:14.351 INFO  --- [restartedMain] o.s.b.a.w.s.WelcomePageHandlerMapping : Adding welcome page: class path resource [static/index.html] 
2019:05:28 18:15:14.882 INFO  --- [restartedMain] o.s.b.d.a.OptionalLiveReloadServer : LiveReload server is running on port 35729 
2019:05:28 18:15:14.976 INFO  --- [restartedMain] o.s.j.e.a.AnnotationMBeanExporter : Registering beans for JMX exposure on startup 
2019:05:28 18:15:14.976 INFO  --- [restartedMain] o.s.j.e.a.AnnotationMBeanExporter : Bean with name 'dataSource' has been autodetected for JMX exposure 
2019:05:28 18:15:14.992 INFO  --- [restartedMain] o.s.j.e.a.AnnotationMBeanExporter : Located MBean 'dataSource': registering with JMX server as MBean [com.zaxxer.hikari:name=dataSource,type=HikariDataSource] 
2019:05:28 18:15:15.070 INFO  --- [restartedMain] o.s.b.w.e.tomcat.TomcatWebServer : Tomcat started on port(s): 8090 (http) with context path '' 
2019:05:28 18:15:15.070 INFO  --- [restartedMain] k.c.t.sapsa.SapSaApplication : Started SapSaApplication in 10.959 seconds (JVM running for 12.467) 
2019:05:28 18:15:15.335 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:18.126z 
2019:05:28 18:15:15.366 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:15:15.460 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:15:15.460 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:15:15.694 INFO  --- [kafka-producer-network-thread | producer-1] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:15:15.725 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:15:18.471 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-41,60,0.82,150,28,26,2019-05-28T18:15:21.340z 
2019:05:28 18:15:18.471 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:15:18.502 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:15:18.502 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:15:18.565 INFO  --- [kafka-producer-network-thread | producer-2] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:15:18.581 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-2] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:15:19.080 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-43,60,0.94,150,28,26,2019-05-28T18:15:21.979z 
2019:05:28 18:15:19.096 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:15:19.127 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:15:19.127 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:15:19.127 INFO  --- [kafka-producer-network-thread | producer-3] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:15:19.127 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-3] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:15:21.716 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:24.569z 
2019:05:28 18:15:21.716 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:15:21.732 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:15:21.732 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:15:28.207 INFO  --- [http-nio-8090-exec-1] o.a.c.c.C.[Tomcat].[localhost].[/] : Initializing Spring FrameworkServlet 'dispatcherServlet' 
2019:05:28 18:15:28.207 INFO  --- [http-nio-8090-exec-1] o.s.web.servlet.DispatcherServlet : FrameworkServlet 'dispatcherServlet': initialization started 
2019:05:28 18:15:28.239 INFO  --- [http-nio-8090-exec-1] o.s.web.servlet.DispatcherServlet : FrameworkServlet 'dispatcherServlet': initialization completed in 32 ms 
2019:05:28 18:15:34.994 INFO  --- [http-nio-8090-exec-5] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:15:35.010 INFO  --- [http-nio-8090-exec-5] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:15:35.042 INFO  --- [http-nio-8090-exec-5] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:15:35.042 INFO  --- [http-nio-8090-exec-5] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:15:35.058 INFO  --- [http-nio-8090-exec-5] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:15:35.058 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-1, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:15:35.058 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-1, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:15:35.073 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-1, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:15:35.089 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-1, groupId=perter-consumer-spark] Successfully joined group with generation 1 
2019:05:28 18:15:35.089 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-1, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:15:35.104 INFO  --- [http-nio-8090-exec-5] o.a.k.c.consumer.internals.Fetcher : [Consumer clientId=consumer-1, groupId=perter-consumer-spark] Resetting offset for partition demo_sensor_spark_w-0 to offset 2620. 
2019:05:28 18:15:36.056 INFO  --- [http-nio-8090-exec-5] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:15:39.973 INFO  --- [http-nio-8090-exec-8] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:15:39.973 INFO  --- [http-nio-8090-exec-8] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:15:39.988 INFO  --- [http-nio-8090-exec-8] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:15:39.988 INFO  --- [http-nio-8090-exec-8] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:15:40.114 INFO  --- [http-nio-8090-exec-8] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:15:40.114 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-2, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:15:40.130 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-2, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:15:40.130 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-2, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:15:40.130 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-2, groupId=perter-consumer-spark] Successfully joined group with generation 3 
2019:05:28 18:15:40.130 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-2, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:15:41.003 INFO  --- [http-nio-8090-exec-8] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:15:42.767 WARN  --- [kafka-producer-network-thread | producer-4] o.a.kafka.clients.NetworkClient : [Producer clientId=producer-4] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:15:42.783 INFO  --- [kafka-producer-network-thread | producer-4] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:15:42.783 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-4] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:15:42.798 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:27.782z 
2019:05:28 18:15:42.798 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:15:42.798 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:15:42.798 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:15:42.862 INFO  --- [kafka-producer-network-thread | producer-5] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:15:42.862 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-5] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:15:42.877 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:31.013z 
2019:05:28 18:15:42.877 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:15:42.877 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:15:42.877 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:15:42.893 INFO  --- [kafka-producer-network-thread | producer-6] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:15:42.893 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-6] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:15:42.893 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-43,60,0.94,150,28,26,2019-05-28T18:15:31.663z 
2019:05:28 18:15:42.893 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:15:42.908 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:15:42.908 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:15:44.983 INFO  --- [http-nio-8090-exec-9] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:15:44.983 INFO  --- [http-nio-8090-exec-9] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:15:44.983 INFO  --- [http-nio-8090-exec-9] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:15:44.983 INFO  --- [http-nio-8090-exec-9] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:16:03.923 WARN  --- [kafka-producer-network-thread | producer-7] o.a.kafka.clients.NetworkClient : [Producer clientId=producer-7] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:16:03.938 INFO  --- [kafka-producer-network-thread | producer-7] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:16:03.955 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-7] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:16:03.956 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:37.462z 
2019:05:28 18:16:03.956 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:16:03.971 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:16:03.971 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:16:05.999 WARN  --- [http-nio-8090-exec-9] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-3, groupId=perter-consumer-spark] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:16:05.999 INFO  --- [http-nio-8090-exec-9] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:16:09.978 INFO  --- [http-nio-8090-exec-7] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:16:09.978 INFO  --- [http-nio-8090-exec-7] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:16:09.994 INFO  --- [http-nio-8090-exec-7] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:16:09.994 INFO  --- [http-nio-8090-exec-7] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:16:14.971 INFO  --- [http-nio-8090-exec-10] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:16:14.971 INFO  --- [http-nio-8090-exec-10] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:16:14.987 INFO  --- [http-nio-8090-exec-10] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:16:14.987 INFO  --- [http-nio-8090-exec-10] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:16:15.003 INFO  --- [http-nio-8090-exec-10] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:16:15.003 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-5, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:16:15.003 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-5, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:16:15.003 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-5, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:16:15.018 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-5, groupId=perter-consumer-spark] Successfully joined group with generation 5 
2019:05:28 18:16:15.018 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-5, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:16:15.035 INFO  --- [http-nio-8090-exec-10] k.c.t.s.controller.KafkaController : /kafka/data records count: 4 
2019:05:28 18:16:15.035 INFO  --- [http-nio-8090-exec-10] k.c.t.s.controller.KafkaController : kafka record: ConsumerRecord(topic = demo_sensor_spark_w, partition = 0, offset = 2620, CreateTime = 1559034950872, serialized key size = -1, serialized value size = 25, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = EE:AB:94:BE:19:A9,0.0,0.0) 
2019:05:28 18:16:15.035 ERROR --- [http-nio-8090-exec-10] k.c.t.s.httpEntity.RestBaseResponse : 4 
2019:05:28 18:16:19.980 INFO  --- [http-nio-8090-exec-2] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:16:19.980 INFO  --- [http-nio-8090-exec-2] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:16:19.996 INFO  --- [http-nio-8090-exec-2] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:16:19.996 INFO  --- [http-nio-8090-exec-2] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:16:19.996 INFO  --- [http-nio-8090-exec-2] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:16:19.996 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-6, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:16:19.996 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-6, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:16:19.996 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-6, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:16:20.011 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-6, groupId=perter-consumer-spark] Successfully joined group with generation 7 
2019:05:28 18:16:20.011 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-6, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:16:21.011 INFO  --- [http-nio-8090-exec-2] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:16:24.974 INFO  --- [http-nio-8090-exec-3] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:16:24.974 INFO  --- [http-nio-8090-exec-3] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:16:24.974 INFO  --- [http-nio-8090-exec-3] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:16:24.974 INFO  --- [http-nio-8090-exec-3] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:16:24.990 INFO  --- [http-nio-8090-exec-3] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:16:24.990 INFO  --- [http-nio-8090-exec-3] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-7, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:16:24.990 INFO  --- [http-nio-8090-exec-3] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-7, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:16:24.990 INFO  --- [http-nio-8090-exec-3] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-7, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:16:25.005 WARN  --- [kafka-producer-network-thread | producer-8] o.a.kafka.clients.NetworkClient : [Producer clientId=producer-8] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:16:25.005 INFO  --- [http-nio-8090-exec-3] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-7, groupId=perter-consumer-spark] Successfully joined group with generation 9 
2019:05:28 18:16:25.005 INFO  --- [http-nio-8090-exec-3] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-7, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:16:25.005 INFO  --- [kafka-producer-network-thread | producer-8] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:16:25.005 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-8] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:16:25.021 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:40.685z 
2019:05:28 18:16:25.021 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:16:25.021 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:16:25.021 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:16:25.037 INFO  --- [kafka-producer-network-thread | producer-9] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:16:25.037 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-9] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:16:25.037 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:43.903z 
2019:05:28 18:16:25.037 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:16:25.053 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:16:25.053 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:16:25.989 INFO  --- [http-nio-8090-exec-3] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:16:27.034 WARN  --- [http-nio-8090-exec-9] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-3, groupId=perter-consumer-spark] Connection to node 3 could not be established. Broker may not be available. 
2019:05:28 18:16:27.050 INFO  --- [http-nio-8090-exec-9] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-3, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:16:27.050 INFO  --- [http-nio-8090-exec-9] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-3, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:16:27.050 INFO  --- [http-nio-8090-exec-9] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-3, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:16:27.065 INFO  --- [http-nio-8090-exec-9] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-3, groupId=perter-consumer-spark] Successfully joined group with generation 11 
2019:05:28 18:16:27.065 INFO  --- [http-nio-8090-exec-9] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-3, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:16:27.065 INFO  --- [http-nio-8090-exec-9] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:16:27.098 INFO  --- [http-nio-8090-exec-4] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:16:27.098 INFO  --- [http-nio-8090-exec-4] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:16:27.098 INFO  --- [http-nio-8090-exec-4] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:16:27.098 INFO  --- [http-nio-8090-exec-4] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:16:27.098 INFO  --- [http-nio-8090-exec-4] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:16:27.113 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-8, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:16:27.113 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-8, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:16:27.113 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-8, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:16:27.113 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-8, groupId=perter-consumer-spark] Successfully joined group with generation 13 
2019:05:28 18:16:27.113 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-8, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:16:28.112 INFO  --- [http-nio-8090-exec-4] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:16:28.127 INFO  --- [http-nio-8090-exec-5] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:16:28.127 INFO  --- [http-nio-8090-exec-5] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:16:28.144 INFO  --- [http-nio-8090-exec-5] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:16:28.144 INFO  --- [http-nio-8090-exec-5] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:16:31.014 WARN  --- [http-nio-8090-exec-7] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-4, groupId=perter-consumer-spark] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:16:31.014 INFO  --- [http-nio-8090-exec-7] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:16:31.030 INFO  --- [http-nio-8090-exec-7] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-4, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:16:31.030 INFO  --- [http-nio-8090-exec-7] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-4, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:16:31.030 INFO  --- [http-nio-8090-exec-7] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-4, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:16:31.030 INFO  --- [http-nio-8090-exec-7] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-4, groupId=perter-consumer-spark] Successfully joined group with generation 15 
2019:05:28 18:16:31.030 INFO  --- [http-nio-8090-exec-7] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-4, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:16:31.045 INFO  --- [http-nio-8090-exec-7] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:16:39.985 INFO  --- [http-nio-8090-exec-10] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:16:39.985 INFO  --- [http-nio-8090-exec-10] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:16:40.001 INFO  --- [http-nio-8090-exec-10] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:16:40.001 INFO  --- [http-nio-8090-exec-10] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:16:40.064 INFO  --- [http-nio-8090-exec-10] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:16:40.064 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-10, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:16:40.064 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-10, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:16:40.064 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-10, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:16:40.080 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-10, groupId=perter-consumer-spark] Successfully joined group with generation 17 
2019:05:28 18:16:40.080 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-10, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:16:40.080 INFO  --- [http-nio-8090-exec-10] k.c.t.s.controller.KafkaController : /kafka/data records count: 2 
2019:05:28 18:16:40.080 INFO  --- [http-nio-8090-exec-10] k.c.t.s.controller.KafkaController : kafka record: ConsumerRecord(topic = demo_sensor_spark_w, partition = 0, offset = 2624, CreateTime = 1559034990923, serialized key size = -1, serialized value size = 25, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = EE:AB:94:BE:19:A9,0.0,0.0) 
2019:05:28 18:16:40.095 ERROR --- [http-nio-8090-exec-10] k.c.t.s.httpEntity.RestBaseResponse : 4 
2019:05:28 18:16:44.963 INFO  --- [http-nio-8090-exec-1] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:16:44.963 INFO  --- [http-nio-8090-exec-1] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:16:44.978 INFO  --- [http-nio-8090-exec-1] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:16:44.978 INFO  --- [http-nio-8090-exec-1] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:16:45.166 INFO  --- [http-nio-8090-exec-1] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:16:45.166 INFO  --- [http-nio-8090-exec-1] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-11, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:16:45.166 INFO  --- [http-nio-8090-exec-1] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-11, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:16:45.166 INFO  --- [http-nio-8090-exec-1] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-11, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:16:45.182 INFO  --- [http-nio-8090-exec-1] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-11, groupId=perter-consumer-spark] Successfully joined group with generation 19 
2019:05:28 18:16:45.182 INFO  --- [http-nio-8090-exec-1] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-11, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:16:45.993 INFO  --- [http-nio-8090-exec-1] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:16:46.088 WARN  --- [kafka-producer-network-thread | producer-10] o.a.kafka.clients.NetworkClient : [Producer clientId=producer-10] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:16:46.088 INFO  --- [kafka-producer-network-thread | producer-10] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:16:46.103 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-10] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:16:46.103 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:24.569z 
2019:05:28 18:16:46.103 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:16:46.103 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:16:46.103 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:16:46.181 INFO  --- [kafka-producer-network-thread | producer-11] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:16:46.181 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-11] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:16:46.181 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:50.994z 
2019:05:28 18:16:46.181 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:16:46.197 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:16:46.197 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:16:46.259 INFO  --- [kafka-producer-network-thread | producer-12] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:16:46.275 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-12] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:16:46.275 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-43,60,0.94,150,28,26,2019-05-28T18:15:31.663z 
2019:05:28 18:16:46.291 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:16:46.291 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:16:46.291 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:16:49.161 WARN  --- [http-nio-8090-exec-5] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-9, groupId=perter-consumer-spark] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:16:49.161 INFO  --- [http-nio-8090-exec-5] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:16:49.161 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-9, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:16:49.177 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-9, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:16:49.177 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-9, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:16:49.177 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-9, groupId=perter-consumer-spark] Successfully joined group with generation 21 
2019:05:28 18:16:49.177 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-9, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:16:49.177 INFO  --- [http-nio-8090-exec-5] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:16:49.209 INFO  --- [http-nio-8090-exec-2] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:16:49.209 INFO  --- [http-nio-8090-exec-2] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:16:49.209 INFO  --- [http-nio-8090-exec-2] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:16:49.209 INFO  --- [http-nio-8090-exec-2] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:16:49.224 INFO  --- [http-nio-8090-exec-2] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:16:49.224 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-12, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:16:49.224 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-12, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:16:49.224 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-12, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:16:49.224 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-12, groupId=perter-consumer-spark] Successfully joined group with generation 23 
2019:05:28 18:16:49.224 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-12, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:16:49.240 INFO  --- [http-nio-8090-exec-2] k.c.t.s.controller.KafkaController : /kafka/data records count: 1 
2019:05:28 18:16:49.240 INFO  --- [http-nio-8090-exec-2] k.c.t.s.controller.KafkaController : kafka record: ConsumerRecord(topic = demo_sensor_spark_w, partition = 0, offset = 2626, CreateTime = 1559035010755, serialized key size = -1, serialized value size = 25, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = EE:AB:94:BE:19:A9,0.0,0.0) 
2019:05:28 18:16:49.240 ERROR --- [http-nio-8090-exec-2] k.c.t.s.httpEntity.RestBaseResponse : 4 
2019:05:28 18:16:49.256 INFO  --- [http-nio-8090-exec-3] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:16:49.256 INFO  --- [http-nio-8090-exec-3] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:16:49.271 INFO  --- [http-nio-8090-exec-3] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:16:49.271 INFO  --- [http-nio-8090-exec-3] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:00.004 INFO  --- [http-nio-8090-exec-4] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:17:00.020 INFO  --- [http-nio-8090-exec-4] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:17:00.020 INFO  --- [http-nio-8090-exec-4] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:00.020 INFO  --- [http-nio-8090-exec-4] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:00.020 INFO  --- [http-nio-8090-exec-4] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:00.020 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-14, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:17:00.020 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-14, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:17:00.020 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-14, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:17:00.035 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-14, groupId=perter-consumer-spark] Successfully joined group with generation 25 
2019:05:28 18:17:00.035 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-14, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:17:00.052 INFO  --- [http-nio-8090-exec-4] k.c.t.s.controller.KafkaController : /kafka/data records count: 1 
2019:05:28 18:17:00.052 INFO  --- [http-nio-8090-exec-4] k.c.t.s.controller.KafkaController : kafka record: ConsumerRecord(topic = demo_sensor_spark_w, partition = 0, offset = 2627, CreateTime = 1559035015843, serialized key size = -1, serialized value size = 25, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = EE:AB:94:BE:19:A9,0.0,0.0) 
2019:05:28 18:17:00.052 ERROR --- [http-nio-8090-exec-4] k.c.t.s.httpEntity.RestBaseResponse : 4 
2019:05:28 18:17:04.981 INFO  --- [http-nio-8090-exec-6] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:17:04.981 INFO  --- [http-nio-8090-exec-6] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:17:04.981 INFO  --- [http-nio-8090-exec-6] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:04.981 INFO  --- [http-nio-8090-exec-6] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:05.357 INFO  --- [http-nio-8090-exec-6] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:05.357 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-15, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:17:05.357 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-15, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:17:05.357 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-15, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:17:05.357 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-15, groupId=perter-consumer-spark] Successfully joined group with generation 27 
2019:05:28 18:17:05.357 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-15, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:17:05.981 INFO  --- [http-nio-8090-exec-6] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:17:07.322 WARN  --- [kafka-producer-network-thread | producer-13] o.a.kafka.clients.NetworkClient : [Producer clientId=producer-13] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:17:07.322 INFO  --- [kafka-producer-network-thread | producer-13] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:07.322 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-13] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:17:07.338 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:54.856z 
2019:05:28 18:17:07.338 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:17:07.354 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:07.354 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:07.354 INFO  --- [kafka-producer-network-thread | producer-14] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:07.354 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-14] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:17:07.354 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:37.462z 
2019:05:28 18:17:07.370 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:17:07.370 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:07.370 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:07.370 INFO  --- [kafka-producer-network-thread | producer-15] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:07.370 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-15] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:17:07.386 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:16:00.667z 
2019:05:28 18:17:07.386 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:17:07.386 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:07.386 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:09.991 INFO  --- [http-nio-8090-exec-8] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:17:09.991 INFO  --- [http-nio-8090-exec-8] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:17:09.991 INFO  --- [http-nio-8090-exec-8] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:09.991 INFO  --- [http-nio-8090-exec-8] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:10.288 WARN  --- [http-nio-8090-exec-3] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-13, groupId=perter-consumer-spark] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:17:10.288 INFO  --- [http-nio-8090-exec-3] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:14.984 INFO  --- [http-nio-8090-exec-10] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:17:14.984 INFO  --- [http-nio-8090-exec-10] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:17:14.984 INFO  --- [http-nio-8090-exec-10] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:14.984 INFO  --- [http-nio-8090-exec-10] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:19.977 INFO  --- [http-nio-8090-exec-1] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:17:19.977 INFO  --- [http-nio-8090-exec-1] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:17:19.993 INFO  --- [http-nio-8090-exec-1] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:19.993 INFO  --- [http-nio-8090-exec-1] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:24.986 INFO  --- [http-nio-8090-exec-5] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:17:24.986 INFO  --- [http-nio-8090-exec-5] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:17:24.986 INFO  --- [http-nio-8090-exec-5] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:24.986 INFO  --- [http-nio-8090-exec-5] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:25.049 INFO  --- [http-nio-8090-exec-5] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:25.049 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-19, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:17:25.049 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-19, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:17:25.049 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-19, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:17:25.049 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-19, groupId=perter-consumer-spark] Successfully joined group with generation 29 
2019:05:28 18:17:25.049 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-19, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:17:25.065 INFO  --- [http-nio-8090-exec-5] k.c.t.s.controller.KafkaController : /kafka/data records count: 2 
2019:05:28 18:17:25.065 INFO  --- [http-nio-8090-exec-5] k.c.t.s.controller.KafkaController : kafka record: ConsumerRecord(topic = demo_sensor_spark_w, partition = 0, offset = 2628, CreateTime = 1559035030884, serialized key size = -1, serialized value size = 25, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = EE:AB:94:BE:19:A9,0.0,0.0) 
2019:05:28 18:17:25.065 ERROR --- [http-nio-8090-exec-5] k.c.t.s.httpEntity.RestBaseResponse : 4 
2019:05:28 18:17:28.403 WARN  --- [kafka-producer-network-thread | producer-16] o.a.kafka.clients.NetworkClient : [Producer clientId=producer-16] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:17:28.403 INFO  --- [kafka-producer-network-thread | producer-16] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:28.403 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-16] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:17:28.419 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:40.685z 
2019:05:28 18:17:28.419 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:17:28.419 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:28.419 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:31.041 WARN  --- [http-nio-8090-exec-8] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-16, groupId=perter-consumer-spark] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:17:31.041 INFO  --- [http-nio-8090-exec-8] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:31.307 WARN  --- [http-nio-8090-exec-3] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-13, groupId=perter-consumer-spark] Connection to node 3 could not be established. Broker may not be available. 
2019:05:28 18:17:31.307 INFO  --- [http-nio-8090-exec-3] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-13, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:17:31.307 INFO  --- [http-nio-8090-exec-3] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-13, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:17:31.307 INFO  --- [http-nio-8090-exec-3] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-13, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:17:31.322 INFO  --- [http-nio-8090-exec-3] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-13, groupId=perter-consumer-spark] Successfully joined group with generation 31 
2019:05:28 18:17:31.322 INFO  --- [http-nio-8090-exec-3] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-13, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:17:31.322 INFO  --- [http-nio-8090-exec-3] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:17:31.369 INFO  --- [http-nio-8090-exec-7] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:17:31.369 INFO  --- [http-nio-8090-exec-7] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:17:31.369 INFO  --- [http-nio-8090-exec-7] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:31.369 INFO  --- [http-nio-8090-exec-7] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:36.002 WARN  --- [http-nio-8090-exec-10] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-17, groupId=perter-consumer-spark] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:17:36.002 INFO  --- [http-nio-8090-exec-10] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:36.002 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-17, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:17:36.002 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-17, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:17:36.002 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-17, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:17:36.018 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-17, groupId=perter-consumer-spark] Successfully joined group with generation 33 
2019:05:28 18:17:36.018 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-17, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:17:36.018 INFO  --- [http-nio-8090-exec-10] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:17:41.011 WARN  --- [http-nio-8090-exec-1] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-18, groupId=perter-consumer-spark] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:17:41.027 INFO  --- [http-nio-8090-exec-1] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:49.452 WARN  --- [kafka-producer-network-thread | producer-17] o.a.kafka.clients.NetworkClient : [Producer clientId=producer-17] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:17:49.452 INFO  --- [kafka-producer-network-thread | producer-17] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:49.452 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-17] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:17:49.467 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:43.903z 
2019:05:28 18:17:49.467 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:17:49.467 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:49.467 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:49.546 INFO  --- [kafka-producer-network-thread | producer-18] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:49.546 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-18] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:17:49.562 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:16:03.898z 
2019:05:28 18:17:49.562 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:17:49.562 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:49.562 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:49.577 INFO  --- [kafka-producer-network-thread | producer-19] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:49.577 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-19] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:17:49.593 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:16:10.352z 
2019:05:28 18:17:49.593 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:17:49.593 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:49.593 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:49.843 INFO  --- [kafka-producer-network-thread | producer-20] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:49.843 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-20] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:17:49.858 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:50.994z 
2019:05:28 18:17:49.858 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:17:49.858 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:49.858 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:49.874 INFO  --- [kafka-producer-network-thread | producer-21] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:49.874 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-21] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:17:49.889 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:16:12.281z 
2019:05:28 18:17:49.889 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:17:49.889 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:49.889 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:49.905 INFO  --- [kafka-producer-network-thread | producer-22] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:49.905 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-22] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:17:49.905 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:16:13.578z 
2019:05:28 18:17:49.921 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:17:49.921 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:49.921 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:49.921 INFO  --- [kafka-producer-network-thread | producer-23] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:49.921 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-23] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:17:49.936 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:54.856z 
2019:05:28 18:17:49.936 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:17:49.936 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:49.936 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:50.139 INFO  --- [kafka-producer-network-thread | producer-24] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:50.139 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-24] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:17:50.155 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:37.462z 
2019:05:28 18:17:50.155 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:17:50.155 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:50.155 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:50.217 INFO  --- [kafka-producer-network-thread | producer-25] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:50.217 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-25] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:17:50.233 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:16:20.022z 
2019:05:28 18:17:50.233 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:17:50.233 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:50.233 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:52.073 WARN  --- [http-nio-8090-exec-8] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-16, groupId=perter-consumer-spark] Connection to node 3 could not be established. Broker may not be available. 
2019:05:28 18:17:52.073 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-16, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:17:52.073 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-16, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:17:52.073 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-16, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:17:52.089 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-16, groupId=perter-consumer-spark] Successfully joined group with generation 35 
2019:05:28 18:17:52.089 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-16, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:17:52.105 INFO  --- [http-nio-8090-exec-8] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:17:52.418 WARN  --- [http-nio-8090-exec-7] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-20, groupId=perter-consumer-spark] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:17:52.418 INFO  --- [http-nio-8090-exec-7] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:52.418 INFO  --- [http-nio-8090-exec-7] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-20, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:17:52.418 INFO  --- [http-nio-8090-exec-7] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-20, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:17:52.418 INFO  --- [http-nio-8090-exec-7] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-20, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:17:52.433 INFO  --- [http-nio-8090-exec-7] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-20, groupId=perter-consumer-spark] Successfully joined group with generation 37 
2019:05:28 18:17:52.433 INFO  --- [http-nio-8090-exec-7] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-20, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:17:52.433 INFO  --- [http-nio-8090-exec-7] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:17:52.464 INFO  --- [http-nio-8090-exec-5] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:17:52.464 INFO  --- [http-nio-8090-exec-5] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:17:52.464 INFO  --- [http-nio-8090-exec-5] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:52.464 INFO  --- [http-nio-8090-exec-5] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:17:52.589 INFO  --- [http-nio-8090-exec-5] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:17:52.589 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-21, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:17:52.589 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-21, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:17:52.589 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-21, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:17:52.605 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-21, groupId=perter-consumer-spark] Successfully joined group with generation 39 
2019:05:28 18:17:52.605 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-21, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:17:52.620 INFO  --- [http-nio-8090-exec-5] k.c.t.s.controller.KafkaController : /kafka/data records count: 2 
2019:05:28 18:17:52.620 INFO  --- [http-nio-8090-exec-5] k.c.t.s.controller.KafkaController : kafka record: ConsumerRecord(topic = demo_sensor_spark_w, partition = 0, offset = 2630, CreateTime = 1559035055871, serialized key size = -1, serialized value size = 25, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = EE:AB:94:BE:19:A9,0.0,0.0) 
2019:05:28 18:17:52.620 ERROR --- [http-nio-8090-exec-5] k.c.t.s.httpEntity.RestBaseResponse : 4 
2019:05:28 18:17:52.636 INFO  --- [http-nio-8090-exec-2] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:17:52.636 INFO  --- [http-nio-8090-exec-2] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:17:52.636 INFO  --- [http-nio-8090-exec-2] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:17:52.636 INFO  --- [http-nio-8090-exec-2] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:02.043 WARN  --- [http-nio-8090-exec-1] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-18, groupId=perter-consumer-spark] Connection to node 3 could not be established. Broker may not be available. 
2019:05:28 18:18:02.043 INFO  --- [http-nio-8090-exec-1] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-18, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:18:02.058 INFO  --- [http-nio-8090-exec-1] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-18, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:18:02.058 INFO  --- [http-nio-8090-exec-1] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-18, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:18:02.058 INFO  --- [http-nio-8090-exec-1] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-18, groupId=perter-consumer-spark] Successfully joined group with generation 41 
2019:05:28 18:18:02.058 INFO  --- [http-nio-8090-exec-1] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-18, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:18:02.058 INFO  --- [http-nio-8090-exec-1] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:18:06.381 INFO  --- [http-nio-8090-exec-9] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:18:06.381 INFO  --- [http-nio-8090-exec-9] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:18:06.381 INFO  --- [http-nio-8090-exec-9] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:06.381 INFO  --- [http-nio-8090-exec-9] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:06.381 INFO  --- [http-nio-8090-exec-9] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:06.381 INFO  --- [http-nio-8090-exec-9] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-23, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:18:06.381 INFO  --- [http-nio-8090-exec-9] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-23, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:18:06.381 INFO  --- [http-nio-8090-exec-9] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-23, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:18:06.396 INFO  --- [http-nio-8090-exec-9] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-23, groupId=perter-consumer-spark] Successfully joined group with generation 43 
2019:05:28 18:18:06.396 INFO  --- [http-nio-8090-exec-9] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-23, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:18:06.396 INFO  --- [http-nio-8090-exec-9] k.c.t.s.controller.KafkaController : /kafka/data records count: 2 
2019:05:28 18:18:06.396 INFO  --- [http-nio-8090-exec-9] k.c.t.s.controller.KafkaController : kafka record: ConsumerRecord(topic = demo_sensor_spark_w, partition = 0, offset = 2632, CreateTime = 1559035075754, serialized key size = -1, serialized value size = 25, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = EE:AB:94:BE:19:A9,0.0,0.0) 
2019:05:28 18:18:06.396 ERROR --- [http-nio-8090-exec-9] k.c.t.s.httpEntity.RestBaseResponse : 4 
2019:05:28 18:18:11.264 WARN  --- [kafka-producer-network-thread | producer-26] o.a.kafka.clients.NetworkClient : [Producer clientId=producer-26] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:18:11.264 INFO  --- [kafka-producer-network-thread | producer-26] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:11.264 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-26] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:18:11.280 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:16:00.667z 
2019:05:28 18:18:11.280 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:18:11.296 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:11.296 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:11.343 INFO  --- [http-nio-8090-exec-6] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:18:11.359 INFO  --- [http-nio-8090-exec-6] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:18:11.359 INFO  --- [http-nio-8090-exec-6] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:11.359 INFO  --- [http-nio-8090-exec-6] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:13.668 WARN  --- [http-nio-8090-exec-2] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-22, groupId=perter-consumer-spark] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:18:13.668 INFO  --- [http-nio-8090-exec-2] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:13.668 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-22, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:18:13.683 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-22, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:18:13.683 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-22, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:18:13.683 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-22, groupId=perter-consumer-spark] Successfully joined group with generation 45 
2019:05:28 18:18:13.683 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-22, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:18:13.699 INFO  --- [http-nio-8090-exec-2] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:18:13.716 INFO  --- [http-nio-8090-exec-3] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:18:13.716 INFO  --- [http-nio-8090-exec-3] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:18:13.716 INFO  --- [http-nio-8090-exec-3] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:13.716 INFO  --- [http-nio-8090-exec-3] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:14.028 INFO  --- [http-nio-8090-exec-3] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:14.028 INFO  --- [http-nio-8090-exec-3] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-25, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:18:14.028 INFO  --- [http-nio-8090-exec-3] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-25, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:18:14.028 INFO  --- [http-nio-8090-exec-3] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-25, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:18:14.028 INFO  --- [http-nio-8090-exec-3] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-25, groupId=perter-consumer-spark] Successfully joined group with generation 47 
2019:05:28 18:18:14.028 INFO  --- [http-nio-8090-exec-3] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-25, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:18:14.043 INFO  --- [http-nio-8090-exec-3] k.c.t.s.controller.KafkaController : /kafka/data records count: 1 
2019:05:28 18:18:14.043 INFO  --- [http-nio-8090-exec-3] k.c.t.s.controller.KafkaController : kafka record: ConsumerRecord(topic = demo_sensor_spark_w, partition = 0, offset = 2634, CreateTime = 1559035095813, serialized key size = -1, serialized value size = 25, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = EE:AB:94:BE:19:A9,0.0,0.0) 
2019:05:28 18:18:14.043 ERROR --- [http-nio-8090-exec-3] k.c.t.s.httpEntity.RestBaseResponse : 4 
2019:05:28 18:18:14.074 INFO  --- [http-nio-8090-exec-10] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:18:14.074 INFO  --- [http-nio-8090-exec-10] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:18:14.074 INFO  --- [http-nio-8090-exec-10] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:14.074 INFO  --- [http-nio-8090-exec-10] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:26.367 INFO  --- [http-nio-8090-exec-8] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:18:26.367 INFO  --- [http-nio-8090-exec-8] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:18:26.367 INFO  --- [http-nio-8090-exec-8] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:26.367 INFO  --- [http-nio-8090-exec-8] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:31.345 INFO  --- [http-nio-8090-exec-7] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:18:31.345 INFO  --- [http-nio-8090-exec-7] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:18:31.360 INFO  --- [http-nio-8090-exec-7] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:31.360 INFO  --- [http-nio-8090-exec-7] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:31.424 INFO  --- [http-nio-8090-exec-7] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:31.424 INFO  --- [http-nio-8090-exec-7] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-28, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:18:31.424 INFO  --- [http-nio-8090-exec-7] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-28, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:18:31.424 INFO  --- [http-nio-8090-exec-7] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-28, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:18:31.424 INFO  --- [http-nio-8090-exec-7] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-28, groupId=perter-consumer-spark] Successfully joined group with generation 49 
2019:05:28 18:18:31.424 INFO  --- [http-nio-8090-exec-7] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-28, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:18:31.439 INFO  --- [http-nio-8090-exec-7] k.c.t.s.controller.KafkaController : /kafka/data records count: 1 
2019:05:28 18:18:31.439 INFO  --- [http-nio-8090-exec-7] k.c.t.s.controller.KafkaController : kafka record: ConsumerRecord(topic = demo_sensor_spark_w, partition = 0, offset = 2635, CreateTime = 1559035100833, serialized key size = -1, serialized value size = 25, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = EE:AB:94:BE:19:A9,0.0,0.0) 
2019:05:28 18:18:31.439 ERROR --- [http-nio-8090-exec-7] k.c.t.s.httpEntity.RestBaseResponse : 4 
2019:05:28 18:18:32.328 WARN  --- [kafka-producer-network-thread | producer-27] o.a.kafka.clients.NetworkClient : [Producer clientId=producer-27] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:18:32.328 INFO  --- [kafka-producer-network-thread | producer-27] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:32.328 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-27] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:18:32.344 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:40.685z 
2019:05:28 18:18:32.344 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:18:32.344 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:32.344 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:32.391 WARN  --- [http-nio-8090-exec-6] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-24, groupId=perter-consumer-spark] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:18:32.391 INFO  --- [http-nio-8090-exec-6] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:32.391 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-24, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:18:32.391 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-24, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:18:32.391 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-24, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:18:32.406 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-24, groupId=perter-consumer-spark] Successfully joined group with generation 51 
2019:05:28 18:18:32.406 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-24, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:18:32.406 INFO  --- [http-nio-8090-exec-6] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:18:32.458 INFO  --- [kafka-producer-network-thread | producer-28] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:32.459 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-28] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:18:32.467 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:16:23.243z 
2019:05:28 18:18:32.468 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:18:32.472 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:32.473 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:32.758 INFO  --- [kafka-producer-network-thread | producer-29] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:32.758 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-29] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:18:32.773 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:15:43.903z 
2019:05:28 18:18:32.773 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:18:32.773 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:32.773 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:32.898 INFO  --- [kafka-producer-network-thread | producer-30] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:32.898 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-30] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:18:32.898 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:16:03.898z 
2019:05:28 18:18:32.914 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:18:32.914 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:32.914 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:32.914 INFO  --- [kafka-producer-network-thread | producer-31] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:32.929 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-31] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:18:32.929 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:16:26.475z 
2019:05:28 18:18:32.929 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:18:32.929 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:32.929 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:35.129 WARN  --- [http-nio-8090-exec-10] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-26, groupId=perter-consumer-spark] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:18:35.129 INFO  --- [http-nio-8090-exec-10] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:35.129 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-26, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:18:35.129 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-26, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:18:35.129 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-26, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:18:35.144 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-26, groupId=perter-consumer-spark] Successfully joined group with generation 53 
2019:05:28 18:18:35.144 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-26, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:18:35.144 INFO  --- [http-nio-8090-exec-10] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:18:35.177 INFO  --- [http-nio-8090-exec-5] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:18:35.177 INFO  --- [http-nio-8090-exec-5] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:18:35.177 INFO  --- [http-nio-8090-exec-5] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:35.177 INFO  --- [http-nio-8090-exec-5] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:41.354 INFO  --- [http-nio-8090-exec-4] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:18:41.354 INFO  --- [http-nio-8090-exec-4] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:18:41.370 INFO  --- [http-nio-8090-exec-4] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:41.370 INFO  --- [http-nio-8090-exec-4] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:41.558 INFO  --- [http-nio-8090-exec-4] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:41.558 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-30, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:18:41.558 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-30, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:18:41.558 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-30, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:18:41.558 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-30, groupId=perter-consumer-spark] Successfully joined group with generation 55 
2019:05:28 18:18:41.558 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-30, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:18:41.574 INFO  --- [http-nio-8090-exec-4] k.c.t.s.controller.KafkaController : /kafka/data records count: 2 
2019:05:28 18:18:41.574 INFO  --- [http-nio-8090-exec-4] k.c.t.s.controller.KafkaController : kafka record: ConsumerRecord(topic = demo_sensor_spark_w, partition = 0, offset = 2636, CreateTime = 1559035115791, serialized key size = -1, serialized value size = 25, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = EE:AB:94:BE:19:A9,0.0,0.0) 
2019:05:28 18:18:41.574 ERROR --- [http-nio-8090-exec-4] k.c.t.s.httpEntity.RestBaseResponse : 4 
2019:05:28 18:18:46.347 INFO  --- [http-nio-8090-exec-2] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:18:46.347 INFO  --- [http-nio-8090-exec-2] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:18:46.347 INFO  --- [http-nio-8090-exec-2] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:46.347 INFO  --- [http-nio-8090-exec-2] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:46.363 INFO  --- [http-nio-8090-exec-2] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:46.363 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-31, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:18:46.363 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-31, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:18:46.363 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-31, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:18:46.363 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-31, groupId=perter-consumer-spark] Successfully joined group with generation 57 
2019:05:28 18:18:46.363 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-31, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:18:46.378 INFO  --- [http-nio-8090-exec-2] k.c.t.s.controller.KafkaController : /kafka/data records count: 1 
2019:05:28 18:18:46.378 INFO  --- [http-nio-8090-exec-2] k.c.t.s.controller.KafkaController : kafka record: ConsumerRecord(topic = demo_sensor_spark_w, partition = 0, offset = 2638, CreateTime = 1559035125848, serialized key size = -1, serialized value size = 25, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = EE:AB:94:BE:19:A9,0.0,0.0) 
2019:05:28 18:18:46.378 ERROR --- [http-nio-8090-exec-2] k.c.t.s.httpEntity.RestBaseResponse : 4 
2019:05:28 18:18:47.393 WARN  --- [http-nio-8090-exec-8] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-27, groupId=perter-consumer-spark] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:18:47.409 INFO  --- [http-nio-8090-exec-8] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:47.425 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-27, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:18:47.425 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-27, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:18:47.425 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-27, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:18:47.425 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-27, groupId=perter-consumer-spark] Successfully joined group with generation 59 
2019:05:28 18:18:47.425 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-27, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:18:47.425 INFO  --- [http-nio-8090-exec-8] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:18:51.388 INFO  --- [http-nio-8090-exec-6] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:18:51.388 INFO  --- [http-nio-8090-exec-6] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:18:51.404 INFO  --- [http-nio-8090-exec-6] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:51.404 INFO  --- [http-nio-8090-exec-6] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:51.404 INFO  --- [http-nio-8090-exec-6] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:51.404 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-32, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:18:51.419 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-32, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:18:51.419 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-32, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:18:51.419 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-32, groupId=perter-consumer-spark] Successfully joined group with generation 61 
2019:05:28 18:18:51.419 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-32, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:18:52.419 INFO  --- [http-nio-8090-exec-6] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:18:53.964 WARN  --- [kafka-producer-network-thread | producer-32] o.a.kafka.clients.NetworkClient : [Producer clientId=producer-32] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:18:53.964 INFO  --- [kafka-producer-network-thread | producer-32] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:53.964 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-32] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:18:53.980 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:16:29.711z 
2019:05:28 18:18:53.980 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:18:53.980 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:53.980 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:53.980 INFO  --- [kafka-producer-network-thread | producer-33] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:53.980 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-33] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:18:53.996 INFO  --- [MQTT Call: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : [topic] mytopic : C5:34:78:B2:45:55,1,1,-61,-42,60,0.88,150,28,26,2019-05-28T18:16:10.352z 
2019:05:28 18:18:53.996 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.ProducerConfig : ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer
 
2019:05:28 18:18:53.996 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:53.996 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:54.012 INFO  --- [kafka-producer-network-thread | producer-34] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:54.012 INFO  --- [MQTT Call: paho1559034913133000000] o.a.k.c.producer.KafkaProducer : [Producer clientId=producer-34] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. 
2019:05:28 18:18:54.012 ERROR --- [MQTT Rec: paho1559034913133000000] kr.co.tipsvalley.sapsa.MqttConn : connectionLost 
2019:05:28 18:18:56.196 WARN  --- [http-nio-8090-exec-5] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-29, groupId=perter-consumer-spark] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:18:56.196 INFO  --- [http-nio-8090-exec-5] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:56.353 INFO  --- [http-nio-8090-exec-10] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:18:56.353 INFO  --- [http-nio-8090-exec-10] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:18:56.368 INFO  --- [http-nio-8090-exec-10] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:18:56.368 INFO  --- [http-nio-8090-exec-10] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:18:56.368 INFO  --- [http-nio-8090-exec-10] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:18:56.368 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-33, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:18:56.368 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-33, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:18:56.368 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-33, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:18:56.384 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-33, groupId=perter-consumer-spark] Successfully joined group with generation 63 
2019:05:28 18:18:56.384 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-33, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:18:57.382 INFO  --- [http-nio-8090-exec-10] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:19:01.361 INFO  --- [http-nio-8090-exec-9] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:19:01.361 INFO  --- [http-nio-8090-exec-9] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:19:01.361 INFO  --- [http-nio-8090-exec-9] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:19:01.361 INFO  --- [http-nio-8090-exec-9] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:19:06.354 INFO  --- [http-nio-8090-exec-4] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:19:06.354 INFO  --- [http-nio-8090-exec-4] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:19:06.370 INFO  --- [http-nio-8090-exec-4] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:19:06.370 INFO  --- [http-nio-8090-exec-4] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:19:11.347 INFO  --- [http-nio-8090-exec-2] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:19:11.347 INFO  --- [http-nio-8090-exec-2] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:19:11.363 INFO  --- [http-nio-8090-exec-2] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:19:11.363 INFO  --- [http-nio-8090-exec-2] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:19:16.356 INFO  --- [http-nio-8090-exec-8] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:19:16.356 INFO  --- [http-nio-8090-exec-8] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:19:16.356 INFO  --- [http-nio-8090-exec-8] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:19:16.356 INFO  --- [http-nio-8090-exec-8] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:19:16.435 INFO  --- [http-nio-8090-exec-8] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:19:16.435 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-37, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:19:16.435 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-37, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:19:16.435 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-37, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:19:16.435 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-37, groupId=perter-consumer-spark] Successfully joined group with generation 65 
2019:05:28 18:19:16.435 INFO  --- [http-nio-8090-exec-8] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-37, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:19:16.451 INFO  --- [http-nio-8090-exec-8] k.c.t.s.controller.KafkaController : /kafka/data records count: 2 
2019:05:28 18:19:16.451 INFO  --- [http-nio-8090-exec-8] k.c.t.s.controller.KafkaController : kafka record: ConsumerRecord(topic = demo_sensor_spark_w, partition = 0, offset = 2639, CreateTime = 1559035140843, serialized key size = -1, serialized value size = 25, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = EE:AB:94:BE:19:A9,0.0,0.0) 
2019:05:28 18:19:16.451 ERROR --- [http-nio-8090-exec-8] k.c.t.s.httpEntity.RestBaseResponse : 4 
2019:05:28 18:19:17.231 WARN  --- [http-nio-8090-exec-5] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-29, groupId=perter-consumer-spark] Connection to node 3 could not be established. Broker may not be available. 
2019:05:28 18:19:17.231 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-29, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:19:17.231 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-29, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:19:17.231 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-29, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:19:17.246 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-29, groupId=perter-consumer-spark] Successfully joined group with generation 67 
2019:05:28 18:19:17.246 INFO  --- [http-nio-8090-exec-5] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-29, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:19:17.246 INFO  --- [http-nio-8090-exec-5] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:19:17.262 INFO  --- [http-nio-8090-exec-3] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:19:17.262 INFO  --- [http-nio-8090-exec-3] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:19:17.262 INFO  --- [http-nio-8090-exec-3] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:19:17.262 INFO  --- [http-nio-8090-exec-3] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:19:22.394 WARN  --- [http-nio-8090-exec-9] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-34, groupId=perter-consumer-spark] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:19:22.394 INFO  --- [http-nio-8090-exec-9] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:19:22.410 INFO  --- [http-nio-8090-exec-9] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-34, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:19:22.410 INFO  --- [http-nio-8090-exec-9] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-34, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:19:22.410 INFO  --- [http-nio-8090-exec-9] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-34, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:19:22.410 INFO  --- [http-nio-8090-exec-9] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-34, groupId=perter-consumer-spark] Successfully joined group with generation 69 
2019:05:28 18:19:22.410 INFO  --- [http-nio-8090-exec-9] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-34, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:19:22.426 INFO  --- [http-nio-8090-exec-9] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:19:26.358 INFO  --- [http-nio-8090-exec-6] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:19:26.358 INFO  --- [http-nio-8090-exec-6] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:19:26.358 INFO  --- [http-nio-8090-exec-6] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:19:26.358 INFO  --- [http-nio-8090-exec-6] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:19:26.373 INFO  --- [http-nio-8090-exec-6] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:19:26.373 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-39, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:19:26.373 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-39, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:19:26.373 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-39, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:19:26.373 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-39, groupId=perter-consumer-spark] Successfully joined group with generation 71 
2019:05:28 18:19:26.373 INFO  --- [http-nio-8090-exec-6] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-39, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:19:27.373 INFO  --- [http-nio-8090-exec-6] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:19:27.404 WARN  --- [http-nio-8090-exec-4] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-35, groupId=perter-consumer-spark] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:19:27.404 INFO  --- [http-nio-8090-exec-4] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:19:27.404 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-35, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:19:27.404 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-35, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:19:27.404 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-35, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:19:27.420 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-35, groupId=perter-consumer-spark] Successfully joined group with generation 73 
2019:05:28 18:19:27.420 INFO  --- [http-nio-8090-exec-4] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-35, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:19:27.420 INFO  --- [http-nio-8090-exec-4] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:19:31.352 INFO  --- [http-nio-8090-exec-10] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:19:31.352 INFO  --- [http-nio-8090-exec-10] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:19:31.352 INFO  --- [http-nio-8090-exec-10] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:19:31.352 INFO  --- [http-nio-8090-exec-10] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:19:31.367 INFO  --- [http-nio-8090-exec-10] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:19:31.367 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-40, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:19:31.367 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-40, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:19:31.367 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-40, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:19:31.383 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-40, groupId=perter-consumer-spark] Successfully joined group with generation 75 
2019:05:28 18:19:31.383 INFO  --- [http-nio-8090-exec-10] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-40, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:19:32.367 INFO  --- [http-nio-8090-exec-10] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:19:32.382 WARN  --- [http-nio-8090-exec-2] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-36, groupId=perter-consumer-spark] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:19:32.382 INFO  --- [http-nio-8090-exec-2] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:19:32.382 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-36, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:19:32.382 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-36, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:19:32.382 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-36, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:19:32.398 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-36, groupId=perter-consumer-spark] Successfully joined group with generation 77 
2019:05:28 18:19:32.398 INFO  --- [http-nio-8090-exec-2] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-36, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:19:32.398 INFO  --- [http-nio-8090-exec-2] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:19:36.346 INFO  --- [http-nio-8090-exec-1] k.c.t.s.controller.KafkaController : /kafka/data start 
2019:05:28 18:19:36.346 INFO  --- [http-nio-8090-exec-1] o.a.k.c.consumer.ConsumerConfig : ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka1:9092, kafka2:9092, kafka3:9092]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = perter-consumer-spark
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
 
2019:05:28 18:19:36.361 INFO  --- [http-nio-8090-exec-1] o.a.k.common.utils.AppInfoParser : Kafka version : 1.1.0 
2019:05:28 18:19:36.361 INFO  --- [http-nio-8090-exec-1] o.a.k.common.utils.AppInfoParser : Kafka commitId : fdcf75ea326b8e07 
2019:05:28 18:19:36.737 INFO  --- [http-nio-8090-exec-1] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:19:36.737 INFO  --- [http-nio-8090-exec-1] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-41, groupId=perter-consumer-spark] Discovered group coordinator kafka2:9092 (id: 2147483645 rack: null) 
2019:05:28 18:19:36.737 INFO  --- [http-nio-8090-exec-1] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-41, groupId=perter-consumer-spark] Revoking previously assigned partitions [] 
2019:05:28 18:19:36.737 INFO  --- [http-nio-8090-exec-1] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-41, groupId=perter-consumer-spark] (Re-)joining group 
2019:05:28 18:19:36.737 INFO  --- [http-nio-8090-exec-1] o.a.k.c.c.i.AbstractCoordinator : [Consumer clientId=consumer-41, groupId=perter-consumer-spark] Successfully joined group with generation 79 
2019:05:28 18:19:36.737 INFO  --- [http-nio-8090-exec-1] o.a.k.c.c.i.ConsumerCoordinator : [Consumer clientId=consumer-41, groupId=perter-consumer-spark] Setting newly assigned partitions [demo_sensor_spark_w-0] 
2019:05:28 18:19:37.361 INFO  --- [http-nio-8090-exec-1] k.c.t.s.controller.KafkaController : /kafka/data records count: 0 
2019:05:28 18:19:38.266 INFO  --- [RMI TCP Connection(2)-127.0.0.1] o.s.b.a.SpringApplicationAdminMXBeanRegistrar$SpringApplicationAdmin : Application shutdown requested. 
2019:05:28 18:19:38.266 INFO  --- [RMI TCP Connection(2)-127.0.0.1] o.s.b.w.s.c.AnnotationConfigServletWebServerApplicationContext : Closing org.springframework.boot.web.servlet.context.AnnotationConfigServletWebServerApplicationContext@70dae9ef: startup date [Tue May 28 18:15:05 KST 2019]; root of context hierarchy 
2019:05:28 18:19:38.281 INFO  --- [RMI TCP Connection(2)-127.0.0.1] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans on shutdown 
2019:05:28 18:19:38.281 INFO  --- [RMI TCP Connection(2)-127.0.0.1] o.s.j.e.a.AnnotationMBeanExporter : Unregistering JMX-exposed beans 
2019:05:28 18:19:38.281 INFO  --- [RMI TCP Connection(2)-127.0.0.1] o.s.o.j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default' 
2019:05:28 18:19:38.281 INFO  --- [RMI TCP Connection(2)-127.0.0.1] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown initiated... 
2019:05:28 18:19:38.298 WARN  --- [http-nio-8090-exec-3] o.a.kafka.clients.NetworkClient : [Consumer clientId=consumer-38, groupId=perter-consumer-spark] Connection to node -3 could not be established. Broker may not be available. 
2019:05:28 18:19:38.298 INFO  --- [http-nio-8090-exec-3] org.apache.kafka.clients.Metadata : Cluster ID: lMEc8hmzSxy0e1Ko1RUCNA 
2019:05:28 18:19:38.298 INFO  --- [RMI TCP Connection(2)-127.0.0.1] com.zaxxer.hikari.HikariDataSource : HikariPool-1 - Shutdown completed. 
